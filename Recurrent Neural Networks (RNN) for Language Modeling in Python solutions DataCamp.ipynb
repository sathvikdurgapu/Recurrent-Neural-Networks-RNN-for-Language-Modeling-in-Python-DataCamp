{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN) for Language Modeling in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Recurrent Neural Networks and Keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the number of parameter of RNN and ANN**\n",
    "\n",
    "In this exercise, you will compare the number of parameters of an artificial neural network (ANN) with the recurrent neural network (RNN) architectures. Here, the vocabulary size is equal to 10,000 for both models.\n",
    "\n",
    "The models have been defined for you with similar architectures of only one layer with 256 units (Dense or RNN) plus the output layer. They are stored on variables ann_model and rnn_model.\n",
    "\n",
    "Use the method .summary() to print the models' architecture and number of parameters and select the correct statement.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The RNN model has few parameters than the ANN model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the predicion for all the sentences\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x>0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame({'sentence': sentences, 'y_pred': pred_sentiment, 'y_true': y_test})\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence to sequence models**\n",
    "\n",
    "In the video exercise, you learned about four types of sequence to sequence models: many-to-one (classification) and many-to-many (text generation, neural machine translation and language models). In this exercise, you have to choose the correct type of model given the following problem description:\n",
    "\n",
    "You are helping your friend who is a specialist in speech recognition. Your friend built a model that can recognize different accents of English, but the model is failing to distinguish homophones - words with the same pronunciation but have different meaning such as \"sea\" vs \"see\" or \"write\" vs \"right\".\n",
    "\n",
    "You propose to use a model that will use the context around the words to identify the semantic meaning of the words. By learning the meaning of the words, the new model would avoid outputs like \"Did you sea that car?\" - it would identify that in this case, the correct word would be \"see\".\n",
    "\n",
    "What type of sequence-to-sequence model is appropriate?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Many-to-many, this problem can be solved with a language model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting used to text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for (i, wd) in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i,wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing text data for model input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i+chars_window])\n",
    "\n",
    "# Print 10 pairs\n",
    "print_examples(sentences, next_chars, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming new text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-1\n",
    "# Instantiate the class\n",
    "model = Sequential(name=\"sequential_model\")\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()\n",
    "\n",
    "# Part-2\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "\n",
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "dense_layer = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the Model class at the end\n",
    "model = Model(inputs=main_input, outputs=dense_layer, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant classes/functions\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your first RNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Method '.evaluate()' shows the loss and accuracy\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: RNN Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploding gradient problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01, momentum=0.9))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "\n",
    "# Part 2\n",
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01, momentum=0.9, clipvalue=3))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanishing gradient problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU cells are better than simpleRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "# Print the old and new model summaries\n",
    "SimpleRNN_model.summary()\n",
    "gru_model.summary()\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n",
    "print(\"GRU model's accuracy:\\t{0}\".format(acc_GRU))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking RNN layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LSTM layer\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('lstm_stack_model_weights.h5')\n",
    "\n",
    "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of parameters comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the embedding layer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=80002, output_dim=wordvec_dim, input_length=200, trainable=True))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the one-hot model\n",
    "model_onehot.summary()\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove pre-trained vectors\n",
    "glove_matrix = load_glove('glove_200d.zip')\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
    "                    embeddings_initializer=Constant(glove_matrix), \n",
    "                    input_length=sentence_len, trainable=False))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings improves performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with embedding\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=max_vocabulary, output_dim=wordvec_dim, input_length=max_len))\n",
    "model.add(SimpleRNN(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('embedding_model_weights.h5')\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_embeddings = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(acc_simpleRNN, acc_embeddings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Better sentiment classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Print the obtained loss and accuracy\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the CNN layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model_cnn.summary()\n",
    "\n",
    "# Load pre-trained weights\n",
    "model_cnn.load_weights('model_weights.h5')\n",
    "\n",
    "# Evaluate the model to get the loss and accuracy values\n",
    "loss, acc = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(loss, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Multi-class classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare label vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-1\n",
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "\n",
    "#Part-2\n",
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "\n",
    "# One-hot encode the indexes\n",
    "Y = to_categorical(numerical_ids)\n",
    "\n",
    "# Check the new shape of the variable\n",
    "print(Y.shape)\n",
    "\n",
    "#Part-3\n",
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "\n",
    "# One-hot encode the indexes\n",
    "Y = to_categorical(numerical_ids)\n",
    "\n",
    "# Check the new shape of the variable\n",
    "print(Y.shape)\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(Y[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-process data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_dataset.data)\n",
    "\n",
    "# Prepare the data\n",
    "prep_data = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "prep_data = pad_sequences(prep_data, maxlen=200)\n",
    "\n",
    "# Prepare the labels\n",
    "prep_labels = to_categorical(news_dataset.target)\n",
    "\n",
    "# Print the shapes\n",
    "print(prep_data.shape)\n",
    "print(prep_labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer learning starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Insert lists of accuracy obtained on the validation set\n",
    "plt.plot(history_no_emb['acc'], marker='o')\n",
    "plt.plot(history_emb['acc'], marker='o')\n",
    "\n",
    "# Add extra descriptions to plot\n",
    "plt.title('Learning with and without pre-trained embedding vectors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['no_embeddings', 'with_embeddings'], loc='upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "w2v_model = Word2Vec.load('bigbang_word2vec.model')\n",
    "\n",
    "# Selected words to check similarities\n",
    "words_of_interest = [\"bazinga\", \"penny\", \"universe\", \"spock\", \"brain\"]\n",
    "\n",
    "# Compute top 5 similar words for each of the words of interest\n",
    "top5_similar_words = []\n",
    "for word in words_of_interest:\n",
    "    top5_similar_words.append(\n",
    "      {word: [item[0] for item in w2v_model.wv.most_similar([word], topn=5)]}\n",
    "    )\n",
    "\n",
    "# Print the similar words\n",
    "print(top5_similar_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring 20 News Groups dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-1\n",
    "# See example article\n",
    "print(news_dataset.data[5])\n",
    "\n",
    "# Part-2\n",
    "# See example article\n",
    "print(news_dataset.data[5])\n",
    "\n",
    "# Transform the text into numerical indexes\n",
    "news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "\n",
    "# Part-3\n",
    "# See example article\n",
    "print(news_dataset.data[5])\n",
    "\n",
    "# Transform the text into numerical indexes\n",
    "news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "\n",
    "# Print the transformed example article\n",
    "print(news_num_indices[5])\n",
    "\n",
    "# Part-4\n",
    "# See example article\n",
    "print(news_dataset.data[5])\n",
    "\n",
    "# Transform the text into numerical indexes\n",
    "news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "\n",
    "# Print the transformed example article\n",
    "print(news_num_indices[5])\n",
    "\n",
    "# Transform the labels into one-hot encoded vectors\n",
    "labels_onehot = to_categorical(news_dataset.target)\n",
    "\n",
    "# Check before and after for the sample article\n",
    "print(\"Before: {0}\\nAfter: {1}\".format(news_dataset.target[5], labels_onehot[5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifying news articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change text for numerical ids and pad\n",
    "X_novel = tokenizer.texts_to_sequences(news_novel.data)\n",
    "X_novel = pad_sequences(X_novel, maxlen=400)\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y_novel = to_categorical(news_novel.target)\n",
    "\n",
    "# Load the model pre-trained weights\n",
    "model.load_weights('classify_news_weights.h5')\n",
    "\n",
    "# Evaluate the model on the new dataset\n",
    "loss, acc = model.evaluate(X_novel, Y_novel, batch_size=64)\n",
    "\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss:\\t{0}\\nAccuracy:\\t{1}\".format(loss, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision-Recall trade-off**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for each class\n",
    "pred_probabilities = model.predict(X_test)\n",
    "\n",
    "# Thresholds at 0.5 and 0.8\n",
    "y_pred_50 = [np.argmax(x) if np.max(x) >= 0.5 else DEFAULT_CLASS for x in pred_probabilities]\n",
    "y_pred_80 = [np.argmax(x) if np.max(x) >= 0.8 else DEFAULT_CLASS for x in pred_probabilities]\n",
    "\n",
    "trade_off = pd.DataFrame({\n",
    "    'Precision_50': precision_score(y_true, y_pred_50, average=None), \n",
    "    'Precision_80': precision_score(y_true, y_pred_80, average=None), \n",
    "    'Recall_50': recall_score(y_true, y_pred_50, average=None), \n",
    "    'Recall_80': recall_score(y_true, y_pred_80, average=None)}, \n",
    "  index=['Class 1', 'Class 2', 'Class 3'])\n",
    "\n",
    "print(trade_off)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision or Recall, that is the question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-1\n",
    "# Compute the precision of the sentiment model\n",
    "prec_sentiment = precision_score(sentiment_y_true, sentiment_y_pred, average=None)\n",
    "print(prec_sentiment)\n",
    "\n",
    "# Part-2\n",
    "# Compute the recall of the sentiment model\n",
    "rec_sentiment = recall_score(sentiment_y_true, sentiment_y_pred, average=None)\n",
    "print(rec_sentiment)\n",
    "\n",
    "# Part-3\n",
    "# Question\n",
    "#You are a manager at a bank responsible for social media analysis with the task to reduce the bad image your bank has obtained \n",
    "# recently because the organization was not identifying its customers' complaints and needs. \n",
    "# You implemented a sentiment analysis model to classify tweets mentioning the bank's name into good (compliments) or bad (complaints).\n",
    "#Imagine the results from the previous steps are the precision and recall measures of the class complaints (check the second item on the precision and recall arrays), which of the following is correct:\n",
    "\n",
    "# Answer: \n",
    "\n",
    "# You want a higher recall score to identify most of the customers complaints instead of higher precision so you can rely on what the model predicted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance on multi-class classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-1\n",
    "# Use the model to predict on new data\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Choose the class with higher probability \n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "\n",
    "# Part-2\n",
    "# Use the model to predict on new data\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Choose the class with higher probability \n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Part-3\n",
    "# Use the model to predict on new data\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Choose the class with higher probability \n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Create the performance report\n",
    "print(classification_report(y_true, y_pred, target_names=news_cat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Sequence to Sequence Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text generation examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context for Sheldon phrase\n",
    "sheldon_context = \"Iâ€™m not insane, my mother had me tested. \"\n",
    "\n",
    "# Generate one Sheldon phrase\n",
    "sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n",
    "\n",
    "# Print the phrase\n",
    "print(sheldon_phrase)\n",
    "\n",
    "# Context for poem\n",
    "poem_context = \"May thy beauty forever remain\"\n",
    "\n",
    "# Print the poem\n",
    "print(generate_poem(poem_model, poem_context))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMT example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text into sequence of indexes and pad\n",
    "X = encode_sequences(sentences)\n",
    "\n",
    "# Print the sequences of indexes\n",
    "print(X)\n",
    "\n",
    "# Translate the sentences\n",
    "translated = translate_many(model, X)\n",
    "\n",
    "# Create pandas DataFrame with original and translated\n",
    "df = pd.DataFrame({'Original': sentences, 'Translated': translated})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict next character**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_char(model, initial_text, chars_window, char_to_index, index_to_char):\n",
    "  \t# Initialize the X vector with zeros\n",
    "    X = initialize_X(initial_text, chars_window, char_to_index)\n",
    "    \n",
    "    # Get next character using the model\n",
    "    next_char = predict_next_char(model, X, index_to_char)\n",
    "\t\n",
    "    return next_char\n",
    "\n",
    "# Define context sentence and print the generated text\n",
    "initial_text = \"I am not insane, \"\n",
    "print(\"Next character: {0}\".format(get_next_char(model, initial_text, 20, char_to_index, index_to_char)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate sentence with context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase(model, initial_text):\n",
    "    # Initialize variables  \n",
    "    res, seq, counter, next_char = initialize_params(initial_text)\n",
    "    \n",
    "    # Loop until stop conditions are met\n",
    "    while counter < 100 and next_char != r'.':\n",
    "      \t# Get next char using the model and append to the sentence\n",
    "        next_char, res, seq = get_next_token(model, res, seq)\n",
    "        # Update the counter\n",
    "        counter = counter + 1\n",
    "    return res\n",
    "  \n",
    "# Create a phrase\n",
    "print(generate_phrase(model, \"I am not insane, \"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the probability scale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial text\n",
    "initial_text = \"Spock and me \"\n",
    "\n",
    "# Define a vector with temperature values\n",
    "temperatures = [0.2, 0.8, 1.0, 3.0, 10.0]\n",
    "\n",
    "# Loop over temperatures and generate phrases\n",
    "for temperature in temperatures:\n",
    "\t# Generate a phrase\n",
    "\tphrase = generate_phrase(model, initial_text, temperature)\n",
    "    \n",
    "\t# Print the phrase\n",
    "\tprint('Temperature {0}: {1}'.format(temperature, phrase))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create vectors of sentences and next characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectors\n",
    "sentences = []\n",
    "next_chars = []\n",
    "# Loop for every sentence\n",
    "for sentence in sheldon.split('\\n'):\n",
    "    # Get 20 previous chars and next char; then shift by step\n",
    "    for i in range(0, len(sentence) - chars_window, step):\n",
    "        sentences.append(sentence[i:i + chars_window])\n",
    "        next_chars.append(sentence[i + chars_window])\n",
    "\n",
    "# Define a Data Frame with the vectors\n",
    "df = pd.DataFrame({'sentence': sentences, 'next_char': next_chars})\n",
    "\n",
    "# Print the initial rows\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the data for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the variables with zeros\n",
    "numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)\n",
    "numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)\n",
    "\n",
    "# Loop for every sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "  # Loop for every character in sentence\n",
    "  for t, char in enumerate(sentence):\n",
    "    # Set position of the character to 1\n",
    "    numerical_sentences[i, t, char_to_index[char]] = 1\n",
    "    # Set next character to 1\n",
    "    numerical_next_chars[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Print the first position of each\n",
    "print(numerical_sentences[0], numerical_next_chars[0], sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the text generation model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Sequential(name=\"LSTM model\")\n",
    "\n",
    "# Add two LSTM layers\n",
    "model.add(LSTM(64, input_shape=input_shape, dropout=0.15, recurrent_dropout=0.15, return_sequences=True, name=\"Input_layer\"))\n",
    "model.add(LSTM(64, dropout=0.15, recurrent_dropout=0.15, return_sequences=False, name=\"LSTM_hidden\"))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(n_vocab, activation='softmax', name=\"Output_layer\"))\n",
    "\n",
    "# Compile and load weights\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.load_weights('model_weights.h5')\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the input text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum length of the sentences\n",
    "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
    "\n",
    "# Transform text to sequence of numerical indexes\n",
    "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(X, maxlen=pt_length, padding='post')\n",
    "\n",
    "# Print first sentence\n",
    "print(pt_sentences[0])\n",
    "\n",
    "# Print transformed sentence\n",
    "print(X[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the output text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variable\n",
    "Y = transform_text_to_sequences(en_sentences, output_tokenizer)\n",
    "\n",
    "# Temporary list\n",
    "ylist = list()\n",
    "for sequence in Y:\n",
    "  \t# One-hot encode sentence and append to list\n",
    "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
    "\n",
    "# Update the variable\n",
    "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
    "\n",
    "# Print the raw sentence and its transformed version\n",
    "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translate Portuguese to English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict many phrases\n",
    "def predict_many(model, sentences, index_to_word, raw_dataset):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Translate the Portuguese sentence\n",
    "        translation = predict_one(model, sentence, index_to_word)\n",
    "        \n",
    "        # Get the raw Portuguese and English sentences\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        \n",
    "        # Print the correct Portuguese and English sentences and the predicted\n",
    "        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\n",
    "predict_many(model, X_test[0:10], en_index_to_word, test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
